class MultiHeadAttentionLayer(nn.Module):
    def __init__(self, hid_dim, n_heads, dropout, device, relative_position=True, atten_cos=False, save_atten=''):
        super().__init__()

        assert hid_dim % n_heads == 0

        # relative_position=False

        self.hid_dim = hid_dim
        self.n_heads = n_heads
        self.head_dim = hid_dim // n_heads
        self.max_relative_position = 2

        self.relative_position = relative_position
        self.atten_cos = atten_cos
        self.save_atten = save_atten

        if self.relative_position:
            self.relative_position_k = RelativePosition(self.head_dim,
                                                        self.max_relative_position)
            self.relative_position_v = RelativePosition(self.head_dim,
                                                    self.max_relative_position)

        self.fc_q = nn.Linear(hid_dim, hid_dim)
        self.fc_k = nn.Linear(hid_dim, hid_dim)
        self.fc_v = nn.Linear(hid_dim, hid_dim)

        self.fc_o = nn.Linear(hid_dim, hid_dim)
        self.dropout = nn.Dropout(dropout)

        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)

    def forward(self, query, key, value, mask=None):
        #query = [batch size, query len, hid dim]
        #key = [batch size, key len, hid dim]
        #value = [batch size, value len, hid dim]
        batch_size = query.shape[0]
        len_k = key.shape[1]
        len_q = query.shape[1]
        len_v = value.shape[1]

        query = self.fc_q(query)
        key = self.fc_k(key)
        value = self.fc_v(value)

        r_q1 = query.view(batch_size, -1, self.n_heads,
                          self.head_dim).permute(0, 2, 1, 3)
        r_k1 = key.view(batch_size, -1, self.n_heads,
                        self.head_dim).permute(0, 2, 1, 3)
        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))

        if self.atten_cos:
            norma = torch.linalg.norm(r_q1, dim=-1, keepdim=True)
            normb = torch.linalg.norm(r_k1, dim=-1, keepdim=True)

            attn1 /= (norma*normb)

        # relative_position
        if self.relative_position:
            r_q2 = query.permute(1, 0,
                                 2).contiguous().view(len_q,
                                                      batch_size * self.n_heads,
                                                      self.head_dim)
            r_k2 = self.relative_position_k(len_q, len_k)
            attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)
            attn2 = attn2.contiguous().view(batch_size, self.n_heads, len_q, len_k)

        if self.relative_position:
            attn = (attn1 + attn2) / self.scale
        else:
            attn = attn1 / self.scale

        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e10)

        # 保存注意力特征图
        attn = torch.softmax(attn, dim=-1)
        if self.save_atten:
            torch.save(attn, self.save_atten)


        attn = self.dropout(attn)


        #attn = [batch size, n heads, query len, key len]
        r_v1 = value.view(batch_size, -1, self.n_heads,
                          self.head_dim).permute(0, 2, 1, 3)
        weight1 = torch.matmul(attn, r_v1)

        if self.relative_position:
            r_v2 = self.relative_position_v(len_q, len_v)
            weight2 = attn.permute(2, 0, 1,
                                   3).contiguous().view(len_q,
                                                        batch_size * self.n_heads,
                                                        len_k)
            weight2 = torch.matmul(weight2, r_v2)
            weight2 = weight2.transpose(0, 1).contiguous().view(
                batch_size, self.n_heads, len_q, self.head_dim)

        if self.relative_position:
            x = weight1 + weight2
        else:
            x = weight1

        #x = [batch size, n heads, query len, head dim]
        x = x.permute(0, 2, 1, 3).contiguous()

        #x = [batch size, query len, n heads, head dim]
        x = x.view(batch_size, -1, self.hid_dim)

        #x = [batch size, query len, hid dim]
        x = self.fc_o(x)

        #x = [batch size, query len, hid dim]
        return x


class bigMoel(nn.Module):
    def __init__(self, max_user_num, fea_dim, dropout=0., drop_atten=0.,
                 relative_position=True, atten_cos=False, save_atten=False, n_heads=5):
        super(bigMoel, self).__init__()

        self.seq_emb = nn.Sequential(nn.Embedding(max_user_num, 20),
                                     nn.Flatten(), nn.Linear(2000, 20),
                                     nn.ReLU())

        self.usr_emb = nn.Embedding(max_user_num, 20)
        self.item_emb = nn.Embedding(max_user_num, 20)

        self.block = nn.Sequential(nn.Linear(fea_dim, 100),
                                   nn.Dropout(dropout), nn.ReLU(),
                                   nn.Linear(100, 10), nn.Dropout(dropout),
                                   nn.ReLU())

        self.score_block = nn.Sequential(nn.Linear(100, 100),
                                         nn.Dropout(dropout), nn.ReLU(),
                                         nn.Linear(100, 10),
                                         nn.Dropout(dropout), nn.ReLU())

        self.fc = nn.Sequential(nn.Linear(80, 20), nn.Dropout(dropout),
                                nn.ReLU(), nn.Linear(20, 2))
        self.usr_att = MultiHeadAttentionLayer(20, n_heads, drop_atten, device, relative_position,
                                               atten_cos=atten_cos,
                                               save_atten='atten1.pth' if save_atten else '')
        # 没有用到！
        self.item_att = MultiHeadAttentionLayer(20, n_heads, drop_atten, device, relative_position,
                                                atten_cos=atten_cos,
                                                save_atten='atten2.pth' if save_atten else '')
        self.flatten = nn.Flatten()

    def forward(self, user_id, item_id, x, seq_input, seq_input_score):
        # print(user_id.shape, item_id.shape, x.shape, seq_input.shape, seq_input_score.shape)
        seq_emb = self.seq_emb(seq_input)
        usr_emb = self.usr_emb(user_id)
        usr_att = self.usr_att(usr_emb, usr_emb, usr_emb)
        usr_att = self.flatten(usr_att)
        # self.flatten

        item_emb = self.item_emb(item_id)
        item_att = self.usr_att(item_emb, item_emb, item_emb)
        item_att = self.flatten(item_att)

        # print(type(x))
        fea_emb = self.block(x)
        score_emb = self.score_block(seq_input_score.float())

        mid_tensor = torch.cat(
            [seq_emb, usr_att, item_att, fea_emb, score_emb], dim=1)
        out_tensor = self.fc(mid_tensor)

        return out_tensor, usr_att, item_att
